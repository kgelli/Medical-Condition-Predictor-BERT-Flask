{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\shrav\\anaconda3\\envs\\bert_training\\python.exe\n",
      "PyTorch: 2.7.1+cpu\n",
      "Transformers: 4.53.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python: {sys.executable}\")\n",
    "import torch, transformers\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages (2.3.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó python setup.py egg_info did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "√ó Encountered error while generating package metadata.\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded: 50000 records\n",
      "‚úÖ Filtered data: 13093 records\n",
      "Conditions distribution:\n",
      "condition\n",
      "Birth Control          8841\n",
      "Depression             2754\n",
      "Diabetes, Type 2        797\n",
      "High Blood Pressure     701\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data with error handling\n",
    "try:\n",
    "    df = pd.read_csv('data/drugsComTrain_raw.tsv', sep='\\t', nrows=50000)  # Reduced for memory\n",
    "    print(f\"‚úÖ Data loaded: {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found. Please check the path.\")\n",
    "    # Alternative path\n",
    "    df = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t', nrows=50000)\n",
    "\n",
    "# Filter and clean data\n",
    "df = df[df['condition'].isin(['Birth Control', 'Depression', 'High Blood Pressure', 'Diabetes, Type 2'])]\n",
    "df = df.dropna(subset=['review'])\n",
    "df = df[df['review'].str.len() > 10]  # Remove very short reviews\n",
    "\n",
    "print(f\"‚úÖ Filtered data: {len(df)} records\")\n",
    "print(f\"Conditions distribution:\\n{df['condition'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    import re\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^<]+?>', '', str(text))\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Truncate very long texts\n",
    "    return text[:512] if len(text) > 512 else text\n",
    "\n",
    "df['cleaned_text'] = df['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Label classes: ['Birth Control' 'Depression' 'Diabetes, Type 2' 'High Blood Pressure']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['encoded_labels'] = label_encoder.fit_transform(df['condition'])\n",
    "print(f\"‚úÖ Label classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train size: 10474, Test size: 2619\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'], \n",
    "    df['encoded_labels'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['encoded_labels']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing training data...\n",
      "üîÑ Tokenizing test data...\n",
      "‚úÖ Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize with progress indication\n",
    "print(\"üîÑ Tokenizing training data...\")\n",
    "train_encodings = tokenizer(\n",
    "    list(X_train), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"üîÑ Tokenizing test data...\")\n",
    "test_encodings = tokenizer(\n",
    "    list(X_test), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets created\n"
     ]
    }
   ],
   "source": [
    "class MedicalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MedicalDataset(train_encodings, y_train)\n",
    "test_dataset = MedicalDataset(test_encodings, y_test)\n",
    "\n",
    "print(\"‚úÖ Datasets created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded\n",
      "‚ÑπÔ∏è  The warning about uninitialized weights is normal - we'll train these layers!\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded\")\n",
    "print(\"‚ÑπÔ∏è  The warning about uninitialized weights is normal - we'll train these layers!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  # Reduced for faster training\n",
    "    per_device_train_batch_size=8,  # Reduced for memory\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # ‚úÖ FIXED: eval_strategy instead of evaluation_strategy\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=0,  # For Windows compatibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy metric\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrav\\AppData\\Local\\Temp\\ipykernel_272028\\1107567922.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "This may take 15-30 minutes depending on your hardware...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3930' max='3930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3930/3930 3:00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.968309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.109719</td>\n",
       "      <td>0.973272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.117983</td>\n",
       "      <td>0.978618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrav\\anaconda3\\envs\\bert_training\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 03:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final Results:\n",
      "   Test Accuracy: 0.9786\n",
      "   Test Loss: 0.1180\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This may take 15-30 minutes depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed!\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   Test Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "    print(f\"   Test Loss: {results['eval_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    print(\"üí° Try reducing batch_size or num_train_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved successfully!\n",
      "üìÅ Saved files:\n",
      "   - ./model/distilbert-drug-review-model/\n",
      "   - ./model/distilbert-drug-review-tokenizer/\n",
      "   - ./model/label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained('./model/distilbert-drug-review-model')\n",
    "    tokenizer.save_pretrained('./model/distilbert-drug-review-tokenizer')\n",
    "    \n",
    "    # Save label encoder\n",
    "    joblib.dump(label_encoder, 'model/label_encoder.pkl')\n",
    "    \n",
    "    print(\"‚úÖ Model saved successfully!\")\n",
    "    print(\"üìÅ Saved files:\")\n",
    "    print(\"   - ./model/distilbert-drug-review-model/\")\n",
    "    print(\"   - ./model/distilbert-drug-review-tokenizer/\")\n",
    "    print(\"   - ./model/label_encoder.pkl\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing trained model:\n",
      "Text: I've been taking this birth control pill for 6 mon...\n",
      "Prediction: Birth Control (confidence: 1.000)\n",
      "\n",
      "Text: This antidepressant helped my depression significa...\n",
      "Prediction: Depression (confidence: 1.000)\n",
      "\n",
      "Text: My blood pressure is well controlled with this med...\n",
      "Prediction: High Blood Pressure (confidence: 0.998)\n",
      "\n",
      "üéâ BERT training pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "def test_prediction(text):\n",
    "    \"\"\"Test the trained model with a sample text\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "            confidence = torch.softmax(outputs.logits, dim=-1).max().item()\n",
    "            \n",
    "        predicted_condition = label_encoder.inverse_transform([prediction.item()])[0]\n",
    "        return predicted_condition, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0.0\n",
    "\n",
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"I've been taking this birth control pill for 6 months with no side effects.\",\n",
    "    \"This antidepressant helped my depression significantly.\",\n",
    "    \"My blood pressure is well controlled with this medication.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing trained model:\")\n",
    "for text in test_texts:\n",
    "    condition, confidence = test_prediction(text)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Prediction: {condition} (confidence: {confidence:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\"üéâ BERT training pipeline completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
